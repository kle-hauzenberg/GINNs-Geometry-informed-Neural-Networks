###################
## Model
###################

# put all reused variables into vars
vars:
  nx: 2 # input is 2Ds
  nz: 2 # dimension of the modulation vector
  ginn_bsize: 9 # usually denoted as bz in the code; i.e. the number of ginn models to train in parallel
  val_plot_grid: [3,3] # if nz>2: [n_first_dims, shapes_per_dim]
  problem_str: 'mbb_beam_2d'
  model_str: 'cond_wire'
  # model_str: 'cond_siren'
  ginn_on: 0
  n_points_surface: 30000
  center_scale_coords: True
  nf_is_density: True
  level_set: 0.5
  warmup: 0
wandb_experiment_name: ${eval:'${vars.problem_str}'+'_'+'${vars.model_str}'}


# useful global variables
nx: ${vars.nx}
nz: ${vars.nz}
ginn_bsize: ${vars.ginn_bsize}
level_set: ${vars.level_set}
nf_is_density: ${vars.nf_is_density}


model:
  nx: ${vars.nx}
  nz: ${vars.nz}
  model_str: ${vars.model_str}
  return_density: ${vars.nf_is_density}
  w0_initial: 18
  w0: 1
  wire_scale: 20
  layers: [32,32,32]  # 3x128 and 3x256 perform similarly (see 67hsix5l vs 2kml4jrp)

reset_zlatents_every_n_epochs: 1  # uncomment to set; not set means the latents are kept fix
latent_sampling:
  nz: ${vars.nz}
  ginn_bsize: ${vars.ginn_bsize}
  z_sample_method: 'uniform'  # any of ['uniform', 'normal', 'equidistant']
  val_plot_grid: ${vars.val_plot_grid}
  z_sample_interval: [0.,1.]

###################
## Data
################### 
data:
  simjeb_ids: []  # 59, 62, 114, 148, 302; has to be [left_corner, right_corner] for the interpolation

###################
## GINN
###################
# Training
ginn_bsize: ${vars.ginn_bsize} # usually denoted as bz in the code; i.e. the number of ginn models to train in parallel
train_plot_max_n_shapes: 16 #${eval:${ginn_bsize}+${eval:len(${simjeb_ids})}} # must not be bigger than ginn_bsize
plot_every_n_epochs: 20
# Validation
valid_every_n_epochs: 200
# for nz>2 the zlatents are not easily displayed. Therefore we plot interpolations along the first n dimensions with subsampling
# for nz \in {1,2} we can plot the zlatents directly

###################
## Loss-balancing
###################
## global switch for GINN losses

## Data losses
lambda_data: 0.0e+1
lambda_lip: ${eval:1.0e-7 * 0}
lambda_dirichlet: ${eval:1.0e1 * 0}
dirichlet_use_surface_points: True
use_config: False
## GINN losses
objective: 'comp'
### local
lambda_if: ${eval:${vars.ginn_on} * 1}
lambda_env: ${eval:${vars.ginn_on} * 1}
lambda_obst: ${eval:${vars.ginn_on} * 0}
lambda_if_normal: ${eval:${vars.ginn_on} * 1}
### global
lambda_scc: 0 #${eval:${vars.ginn_on} * 1}
lambda_eikonal: ${eval:${vars.ginn_on} * 1}
lambda_curv: 0
lambda_div: ${eval:int(${vars.ginn_bsize}> 1) * 0}
lambda_chamfer_div: ${eval:int(${vars.ginn_bsize}> 1) * 1}
lambda_comp: 1
lambda_vol: 1
lambda_vol_gpu: 0
lambda_wasserstein_div: 0


max_curv: 0 # 180 # 500
# max_div: -0.1 # for min-diversity
max_div: -0.1 # for min-diversity
# max_div: -2.0 # for leinster-diversity
scale_curv: 1.0
scale_chamfer_div: 1.0
start_curv: 0
start_comp: ${vars.warmup}

scale_wasserstein_div: 10.0


###################
## Curvature
###################



###################
## SCC algorithm
###################
ph:
  nx: ${vars.nx}
  problem_str: ${vars.problem_str}
  ginn_bsize: ${vars.ginn_bsize}
  iso_level: 0.5  # implicitly sets the thickness
  is_density: ${vars.nf_is_density}
  ph_1_hole_level: 0.1
  ph_loss_maxdim: 0
  ph_loss_target_betti: [1, 0, 0]
  scc_penalty_norm_eps: 1.0e-4
  scc_n_grid_points: 128
  evaluate_ph_grid_per_shape: True  # Set to True to save memory, but it's slower

###################
## Surface points
###################
surface:
  nx: ${vars.nx}
  problem_str: ${vars.problem_str}
  ginn_bsize: ${vars.ginn_bsize}
  n_points_surface: ${vars.n_points_surface}
  level_set: 0.5
  recompute_every_n_epochs: 1
  do_uniform_resampling: False
  equidistant_init_grid: True
  bin_search_steps: 10

###################
## DIVERSITY
###################
# div_neighbor_agg_fn: 'leinster'
leinster_temp: 1.0
leinster_q: 2
diversity_pts_source: domain # domain, surface
div_norm_order: 2
chamfer_div_eps: 1.0e-9 # TODO does this make a difference?
chamfer_p: 1
chamfer_div_eps: 1.0e-9
chamfer_div_subsample: 8192
wasserstein_subsample: 5000
###################
## Metrics computation
###################



###################
## LOGGING
###################
log_level: 'DEBUG'

## Multi-processing
num_workers: 8  # 0 means no multi-processing

## Plotting
plot:
  level_set: ${vars.level_set}
  val_plot_grid: ${vars.val_plot_grid}
  fig_size: [12, 7]
  show_colorbar: True

meshing:
  level_set: ${vars.level_set}
  nf_is_density: ${vars.nf_is_density}


## Specific plots
plot_shape: True
val_plot_shape: True
plot_surface_points: True
plot_ph_diagram: True
plot_grad_field: True

## Timer
timer:
  print: False
  accumulate: True ## record and print the accumulated timings at the end

problem_sampling:
  ## Number of points to sample
  nx: ${vars.nx}
  n_points_domain: 100000  # used for eikonal loss
  n_points_envelope: 2048
  n_points_interfaces: 512
  n_points_normals: 512
  n_points_surface: ${vars.n_points_surface}
  n_points_rotation_symmetric: 512

###################
## Problem
###################
problem:
  problem_str: ${vars.problem_str}
  simjeb_root_dir: 'GINN/simJEB/data'
  plot_2d_resolution: 100 # means 100x100
  mc_resolution: 128
  mc_chunks: 1
  mesh_reduction: 0.9  # 0.9 means 10% of the original resolution; 0.0 means no change
  width: 3
  height: 1
  center_scale_coords: ${vars.center_scale_coords}




###################
## FEM
###################
FEM:
  center_scale_coords: ${vars.center_scale_coords}
  problem_str: ${vars.problem_str}
  nx: ${vars.nx}
  resolution: [180,60]
  p: 3  # default SIMP penalty
  vol_frac: 0.51
  filter_radius: 0.07
  top_max_num_workers: 16
  normalize_compliance: False  # if True, the compliance is normalized by the initial compliance
  use_filters: False
  vol_loss: mse_pushdown
  add_x_offset: True

###################
## Topology optimization
###################
# FeniTop doubles every 100 iterations; we increase beta smoothly
beta_sched:
  init: 1.
  interval: 1 # increase beta every interval epochs
  max: 100.
  stop_iter: 800

p_sched:
  init: 1.
  interval: 1 # increase beta every interval epochs
  max: 3.
  start_iter: 100000  # turned off by default
  stop_iter: ${eval:${vars.warmup}+500}

###################
## Training
###################
## Outer optimization loop
max_epochs: 1200
lr: 0.0001
## optimizer
opt: adam  # adam, lbfgs, adam_lbfgs, adam_nncg, adam_lbfgs_nncg
## scheduler
use_scheduler: False
scheduler_gamma: 0.5
decay_steps: 5000
## gradient clipping
grad_clipping_on: True
grad_clip: 0.5
## autoclip: https://arxiv.org/abs/2007.14469
auto_clip_on: True
auto_clip_percentile: 0.8
auto_clip_hist_len: 50
auto_clip_min_len: 50

## Weight saving
# model_save_path: '_saved_models'
model_save_path: 'path/to/dir'
save_every_n_epochs: 10
overwrite_existing_saved_model: True
save_optimizer: False

## Weight loading
load_mos: False  # load model, optimizer, scheduler
load_model: False
load_optimizer: False
load_scheduler: False
# model_load_wandb_id: bjyaldf4  # nz=2
model_load_wandb_id: 73zxvbgr  # nz=3
# model_load_wandb_id: a575ptpe # nz=4
